# System Architecture

## ğŸ“Š High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          USER INPUT                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Natural Language Query  â”‚  Structured Input  â”‚  Config File       â”‚
â”‚  "Find tech founders"    â”‚  --company Tesla   â”‚  search.json       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        main.py (CLI)                               â”‚
â”‚  â€¢ Parses command line arguments                                   â”‚
â”‚  â€¢ Routes to appropriate search method                             â”‚
â”‚  â€¢ Displays results and statistics                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   lead_scraper.py (Orchestrator)                   â”‚
â”‚  â€¢ Coordinates all scraping components                             â”‚
â”‚  â€¢ Manages search flow                                             â”‚
â”‚  â€¢ Collects and stores lead data                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NLP Extractor       â”‚  Search Scraper      â”‚  Profile Scrapers    â”‚
â”‚  (Natural Language)  â”‚  (Google Search)     â”‚  (LinkedIn, etc.)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Parse text         â”‚ â€¢ Query Google       â”‚ â€¢ Scrape LinkedIn    â”‚
â”‚ â€¢ Extract criteria   â”‚ â€¢ Extract results    â”‚ â€¢ Parse profiles     â”‚
â”‚ â€¢ Generate queries   â”‚ â€¢ Filter URLs        â”‚ â€¢ Extract emails     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Web Scraper (Selenium)                         â”‚
â”‚  â€¢ Chrome browser automation                                       â”‚
â”‚  â€¢ Page loading and waiting                                        â”‚
â”‚  â€¢ Anti-detection measures                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Exporter (Output)                          â”‚
â”‚  â€¢ CSV export (flattened data)                                     â”‚
â”‚  â€¢ JSON export (full structure)                                    â”‚
â”‚  â€¢ Excel export (formatted)                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         OUTPUT FILES                               â”‚
â”‚  output/leads_20260115_143052.csv                                  â”‚
â”‚  output/leads_20260115_143052.json                                 â”‚
â”‚  output/leads_20260115_143052.xlsx                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ Component Details

### 1. **Main Entry Point** (`main.py`)

```
main.py
â”œâ”€â”€ Parses CLI arguments (--query, --names, --company, etc.)
â”œâ”€â”€ Loads config files if specified
â”œâ”€â”€ Creates LeadScraper instance
â”œâ”€â”€ Routes to appropriate search method
â””â”€â”€ Displays results and exports data
```

**Key Functions:**
- `parse_arguments()` - Parse CLI args
- `load_config_file()` - Load JSON config
- `main()` - Main execution flow

---

### 2. **Lead Scraper** (`lead_scraper.py`)

```
LeadScraper
â”œâ”€â”€ __init__() - Initialize all scrapers
â”œâ”€â”€ search_leads_natural_language() - NLP-based search â­ NEW
â”œâ”€â”€ search_leads_by_name() - Search specific people
â”œâ”€â”€ search_leads_by_company() - Search by company
â”œâ”€â”€ search_leads_by_criteria() - Generic search
â””â”€â”€ export_leads() - Export results
```

**Manages:**
- WebScraper instance
- SearchScraper instance
- LinkedInScraper instance
- ProfileScraper instance
- Lead data collection

---

### 3. **NLP Extractor** (`utils/nlp_extractor.py`) â­ NEW

```
NLPExtractor
â”œâ”€â”€ extract_criteria() - Main extraction method
â”œâ”€â”€ _extract_positions() - Find job titles
â”œâ”€â”€ _extract_company_type() - Identify company type
â”œâ”€â”€ _extract_industry() - Find industry
â”œâ”€â”€ _extract_location() - Detect location
â”œâ”€â”€ _extract_team_size() - Parse team size
â”œâ”€â”€ _extract_founding_year() - Get founding dates
â”œâ”€â”€ generate_search_queries() - Create search strings
â””â”€â”€ format_criteria_summary() - Display extracted info
```

**Extracts:**
- Job positions (CEO, Founder, etc.)
- Company types (startup, SaaS, etc.)
- Industries (tech, fintech, etc.)
- Locations (SF, NYC, etc.)
- Team sizes (2-5, 10-20, etc.)
- Founding dates (last 2 years, etc.)

---

### 4. **Web Scraper** (`utils/web_scraper.py`)

```
WebScraper
â”œâ”€â”€ init_driver() - Start Chrome/Selenium
â”œâ”€â”€ get_page_source() - Load page with Selenium
â”œâ”€â”€ get_page_with_requests() - Load page with requests
â”œâ”€â”€ parse_html() - Parse with BeautifulSoup
â””â”€â”€ close() - Clean up browser
```

**Features:**
- Headless Chrome
- Random user agents
- Configurable delays
- Automatic ChromeDriver installation

---

### 5. **Search Scraper** (`scrapers/search_scraper.py`)

```
SearchScraper
â”œâ”€â”€ google_search() - Perform Google search
â”œâ”€â”€ search_for_person() - Search for person + company/title
â”œâ”€â”€ search_linkedin_profiles() - LinkedIn-specific search
â””â”€â”€ search_company_employees() - Find employees
```

**Does:**
- Submits queries to Google
- Parses search results
- Extracts titles, URLs, snippets
- Filters for LinkedIn profiles

---

### 6. **LinkedIn Scraper** (`scrapers/linkedin_scraper.py`)

```
LinkedInScraper
â”œâ”€â”€ login() - Optional LinkedIn login
â”œâ”€â”€ scrape_profile() - Main profile scraper
â”œâ”€â”€ _extract_experience() - Get work history
â””â”€â”€ _extract_education() - Get education
```

**Extracts:**
- Name
- Headline (current position)
- Location
- About/bio section
- Work experience
- Education
- Emails (if public)

---

### 7. **Profile Scraper** (`scrapers/profile_scraper.py`)

```
ProfileScraper
â”œâ”€â”€ scrape_website_for_contacts() - Scrape any website
â”œâ”€â”€ scrape_about_page() - Scrape about/team pages
â”œâ”€â”€ _extract_phone_numbers() - Find phone numbers
â”œâ”€â”€ _extract_social_links() - Find social media
â”œâ”€â”€ _find_contact_page() - Locate contact page
â””â”€â”€ _extract_team_members() - Parse team members
```

**Finds:**
- Email addresses
- Phone numbers
- Social media links (Twitter, GitHub, etc.)
- Contact pages
- Team member info

---

### 8. **Email Extractor** (`utils/email_extractor.py`)

```
EmailExtractor
â”œâ”€â”€ extract_emails() - Main extraction method
â”œâ”€â”€ extract_from_html() - Extract from HTML
â”œâ”€â”€ _is_valid_email() - Validate email
â””â”€â”€ EMAIL_PATTERN - Regex pattern
```

**Features:**
- Regex-based extraction
- Email validation
- False positive filtering
- HTML entity decoding

---

### 9. **Data Exporter** (`data_exporter.py`)

```
DataExporter
â”œâ”€â”€ export_to_csv() - CSV export
â”œâ”€â”€ export_to_json() - JSON export
â”œâ”€â”€ export_to_excel() - Excel export
â””â”€â”€ export_summary() - Generate statistics
```

**Exports:**
- CSV (spreadsheet format)
- JSON (full structured data)
- Excel (formatted workbook)
- Summary statistics

---

## ğŸ”„ Data Flow

### Example: Natural Language Search

```
1. User Input
   â†“
   "Find startup founders in SF with 2-5 team members"

2. NLP Extraction
   â†“
   {
     positions: ['Founder'],
     company_type: 'startup',
     location: 'San Francisco',
     team_size: {min: 2, max: 5}
   }

3. Query Generation
   â†“
   [
     "Founder startup San Francisco",
     "site:linkedin.com/in/ Founder startup"
   ]

4. Google Search
   â†“
   [
     {url: "linkedin.com/in/johndoe", title: "John Doe - Founder"},
     {url: "linkedin.com/in/janesmith", title: "Jane Smith - Co-Founder"},
     ...
   ]

5. Profile Scraping
   â†“
   {
     name: "John Doe",
     current_position: "Founder",
     current_company: "TechStartup Inc",
     location: "San Francisco, CA",
     emails: ["john@techstartup.com"],
     ...
   }

6. Data Export
   â†“
   output/leads_20260115_143052.csv
```

## ğŸ›¡ï¸ Anti-Detection Layer

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Anti-Detection Measures        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Random User Agents                â”‚
â”‚   â†’ Looks like different browsers   â”‚
â”‚                                     â”‚
â”‚ â€¢ Delays Between Requests           â”‚
â”‚   â†’ 2 second default (configurable) â”‚
â”‚   â†’ Randomized timing               â”‚
â”‚                                     â”‚
â”‚ â€¢ Headless Browser                  â”‚
â”‚   â†’ No visible window               â”‚
â”‚   â†’ Faster performance              â”‚
â”‚                                     â”‚
â”‚ â€¢ Request Throttling                â”‚
â”‚   â†’ Limits on concurrent requests   â”‚
â”‚   â†’ Respects rate limits            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âš™ï¸ Configuration System

```
config.py
â”œâ”€â”€ Loads from .env file
â”œâ”€â”€ Default values for all settings
â””â”€â”€ Exports Config class

Settings:
â€¢ HEADLESS_MODE (true/false)
â€¢ TIMEOUT (seconds)
â€¢ DELAY_BETWEEN_REQUESTS (seconds)
â€¢ OUTPUT_FORMAT (csv/json/excel)
â€¢ OUTPUT_DIRECTORY (./output)
â€¢ LINKEDIN_EMAIL (optional)
â€¢ LINKEDIN_PASSWORD (optional)
```

## ğŸ“¦ Dependencies

```
Core:
â”œâ”€â”€ selenium - Browser automation
â”œâ”€â”€ beautifulsoup4 - HTML parsing
â”œâ”€â”€ requests - HTTP requests
â””â”€â”€ lxml - XML/HTML parser

Data:
â”œâ”€â”€ pandas - Data manipulation
â”œâ”€â”€ openpyxl - Excel export
â””â”€â”€ python-dotenv - Environment config

Utilities:
â”œâ”€â”€ webdriver-manager - Auto ChromeDriver
â”œâ”€â”€ fake-useragent - User agent rotation
â””â”€â”€ email-validator - Email validation
```

## ğŸ¯ Design Patterns Used

1. **Context Manager Pattern**
   - `with LeadScraper() as scraper:`
   - Automatic resource cleanup

2. **Factory Pattern**
   - Query generation from criteria
   - Dynamic scraper selection

3. **Strategy Pattern**
   - Different search strategies
   - Multiple export formats

4. **Observer Pattern**
   - Progress reporting
   - Status updates

5. **Singleton Pattern**
   - Config class
   - WebDriver instance

## ğŸ“Š Performance Characteristics

```
Operation                 Time              Notes
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
NLP Extraction           ~0.01s            Regex-based, fast
Query Generation         ~0.01s            String operations
Google Search            ~2-5s/query       Network + parsing
LinkedIn Profile         ~2-5s/profile     Network + parsing
Email Extraction         ~0.1s/page        Regex + validation
Data Export (CSV)        ~0.1s/100 leads   File I/O
Data Export (Excel)      ~0.5s/100 leads   Formatting overhead

Total for 10 leads:      ~30-60 seconds
Total for 50 leads:      ~2-5 minutes
Total for 100 leads:     ~5-10 minutes
```

## ğŸ”’ Security Considerations

1. **No Credentials Stored**
   - LinkedIn login is optional
   - Credentials only in .env (gitignored)

2. **Public Data Only**
   - Only scrapes publicly accessible info
   - Respects robots.txt

3. **Rate Limiting**
   - Built-in delays
   - Configurable throttling

4. **Data Privacy**
   - No data sent to third parties
   - All processing local

## ğŸš€ Scalability

```
Current:
â€¢ Single-threaded
â€¢ Sequential processing
â€¢ Local storage

Potential Improvements:
â€¢ Multi-threading for parallel scraping
â€¢ Database backend for large datasets
â€¢ Proxy rotation for higher volume
â€¢ Caching to avoid re-scraping
â€¢ Distributed processing
```

## ğŸ“ Error Handling

```
Try/Catch at Multiple Levels:
â”œâ”€â”€ Network errors (timeout, connection)
â”œâ”€â”€ Parsing errors (missing elements)
â”œâ”€â”€ Validation errors (invalid emails)
â””â”€â”€ File I/O errors (disk full, permissions)

Graceful Degradation:
â€¢ Missing fields â†’ Empty string
â€¢ Failed profile â†’ Skip and continue
â€¢ No emails found â†’ Empty list
```

---

**This architecture enables:**
âœ… Flexible input (CLI, API, config)
âœ… Natural language understanding
âœ… Multiple data sources
âœ… Robust error handling
âœ… Multiple export formats
âœ… Easy extension and customization
